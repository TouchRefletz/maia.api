import { gerarConteudoEmJSONComImagemStream } from "../api/worker.js";
import * as MemoryService from "../services/memory-service.js"; // Import MemoryService
import { findBestQuestion } from "../services/question-service.js"; // Import question service
import { fileToBase64 } from "../utils/file-utils.js";
import { parseStreamedJSON } from "../utils/json-stream-parser.js";
import { getGenerationParams, getModeConfig } from "./config.js";
import {
  getSystemPromptRaciocinio,
  getSystemPromptRapido,
  getSystemPromptScaffolding,
} from "./prompts/chat-system-prompt.js";
import { determineFinalMode } from "./router.js";
import { CHAT_RESPONSE_SCHEMA, SCAFFOLDING_STEP_SCHEMA } from "./schemas.js"; // Import schema
import { ScaffoldingService } from "./services/scaffolding-service.js"; // Import ScaffoldingService
import { ChatStorageService } from "../services/chat-storage.js"; // Import Persistence Service

/**
 * Pipeline principal - escolhe e executa o pipeline correto
 */
export async function runChatPipeline(
  selectedMode,
  message,
  attachments = [],
  context = {},
) {
  // 1. === PERSISTENCE & INIT ===
  // Gerencia cria√ß√£o de chat se n√£o existir ID
  let chatId = context.chatId;
  let isNewChat = false;

  if (!chatId) {
    try {
      const newChat = await ChatStorageService.createNewChat(
        message,
        attachments,
      );
      chatId = newChat.id;
      context.chatId = chatId; // Atualiza contexto
      isNewChat = true;

      // Notifica UI sobre novo chat
      if (context.onChatCreated) context.onChatCreated(newChat);
    } catch (err) {
      console.warn("[Pipeline] Falha ao criar chat no storage:", err);
    }
  } else {
    // Persiste mensagem do usu√°rio em chat existente
    try {
      await ChatStorageService.addMessage(chatId, "user", message, attachments);
    } catch (err) {
      console.warn("[Pipeline] Falha ao salvar mensagem do user:", err);
    }
  }

  // 2. === MEMORY SYSTEM INTEGRATION (PRE-ROUTING) ===
  // Buscamos mem√≥ria ANTES do router para dar contexto √† decis√£o
  let additionalContextMessage = "";
  let memoryContextForRouter = "";

  try {
    console.log("[Pipeline] üß† Consultando Mem√≥ria Contextual...");
    // Update UI: "Recuperando informa√ß√µes..."
    if (context.onProcessingStatus) {
      context.onProcessingStatus("loading", "Recuperando informa√ß√µes");
    }

    const memoryFacts = await MemoryService.queryContext(message);

    console.log("[Pipeline] üß† Mem√≥ria Contextual encontrada:", memoryFacts);

    if (memoryFacts.length > 0) {
      // S√≠ntese de contexto via LLM para gerar diretivas comportamentais
      const contextString = await MemoryService.synthesizeContext(
        memoryFacts,
        message,
        context.apiKey,
        attachments,
        { signal: context.signal }, // Pass signal
      );

      if (contextString) {
        additionalContextMessage += `\n\n${contextString}\n`;
        // Contexto limpo para o router (sem quebras excessivas)
        memoryContextForRouter = contextString;

        console.log(
          "[Pipeline] üß† Diretivas de mem√≥ria injetadas:",
          contextString,
        );
        // Update UI: "Mem√≥rias recuperadas!" (System Message + Status)
        if (context.onProcessingStatus) {
          // Check abort before updating UI
          if (context.signal?.aborted)
            throw new DOMException("Aborted", "AbortError");

          context.onProcessingStatus("loading", "Escolhendo modo de execu√ß√£o");
          // Passamos o objeto completo de detalhes para a UI renderizar o bloco expans√≠vel
          const memoryContent = {
            title: "Mem√≥rias recuperadas",
            facts: memoryFacts,
            summary: contextString,
          };
          context.onProcessingStatus("memory_found", memoryContent);

          // [PERSISTENCE] Salvar evento de mem√≥ria no hist√≥rico
          if (chatId) {
            ChatStorageService.addMessage(chatId, "system", {
              type: "memory_found",
              ...memoryContent,
            }).catch((err) =>
              console.warn("[Pipeline] Erro ao salvar mem√≥ria:", err),
            );
          }
        }
      }
    }
  } catch (err) {
    if (err.name === "AbortError") throw err; // Propagate abort
    console.warn("[Pipeline] ‚ö†Ô∏è Erro no sistema de mem√≥ria:", err);
  }

  // 2. Determina o modo final (Agora com CONTEXTO de mem√≥ria)
  // Check abort
  if (context.signal?.aborted) throw new DOMException("Aborted", "AbortError");

  // Inicializa conjunto de queries se n√£o existir
  if (!context.previousQueries) {
    context.previousQueries = [];
  }

  const { finalMode, wasRouted, routerResult } = await determineFinalMode(
    selectedMode,
    message,
    attachments,
    memoryContextForRouter,
    {
      previousQueries: context.previousQueries,
      signal: context.signal, // Pass signal to router
      apiKey: context.apiKey,
    },
  );

  // LOGICA SCAFFOLDING
  if (finalMode === "scaffolding") {
    const decision = ScaffoldingService.decidirProximoStatus();

    // Tenta extrair dados do contexto ou mensagem
    // Se for o PRIMEIRO passo gerado pelo chat normal, n√£o temos hist√≥rico ainda,
    // mas o ScaffoldingService lida com isso.
    const questaoAlvo = {
      questao: message, // Assume que a msg do user √© o t√≥pico/quest√£o
      resposta_correta: "N√£o definido",
    };

    // Gera o prompt usando a nova l√≥gica robusta
    const promptRefinado = ScaffoldingService.generateStepPrompt(
      questaoAlvo,
      decision,
      [], // Come√ßa sem hist√≥rico linear no chat principal (o loop slide cuida do resto)
    );

    // Substitui a mensagem original pelo prompt estruturado para garantir o JSON correto
    // OU apenas apenda as instru√ß√µes. O prompt refinado j√° √© completo ("Voc√™ √© um tutor..."),
    // ent√£o idealmente ele substitui ou domina o contexto.
    additionalContextMessage += "\n\n" + promptRefinado;

    console.log(
      `[Pipeline] üèóÔ∏è Scaffolding: Decis√£o System=${decision ? "V" : "F"}, Prompt Refinado Injected.`,
    );
  }

  // Verifica tb se precisa buscar quest√£o (AGORA RODA JUNTO COM SCAFFOLDING SE NECESS√ÅRIO)
  if (wasRouted && routerResult?.busca_questao) {
    console.log(
      "[Pipeline] üîé Router solicitou busca de quest√£o:",
      routerResult.busca_questao,
    );
    try {
      const questionData = await findBestQuestion({
        query: routerResult.busca_questao.conteudo,
        ...routerResult.busca_questao.props,
      });

      if (questionData) {
        console.log(
          "[Pipeline] ‚úÖ Quest√£o encontrada e injetada no contexto:",
          questionData.id,
        );

        // REGISTRA QUERY USADA PARA N√ÉO REPETIR
        context.previousQueries.push(routerResult.busca_questao.conteudo);

        // Injeta a quest√£o no fluxo como um SYSTEM MESSAGE disfar√ßado ou append no user message
        // Para garantir que o modelo use, vamos adicionar explicitamente
        additionalContextMessage += `\n\n[SISTEMA - DADOS INJETADOS]: O usu√°rio solicitou uma quest√£o. Use os dados abaixo para gerar o bloco 'questao' na resposta. N√£o invente, use estes dados:\n${JSON.stringify(questionData.fullData)}`;

        // Opcional: For√ßar modo racioc√≠nio se a quest√£o for complexa? O router j√° deve ter decidido ALTA complexidade.
      }
    } catch (err) {
      console.warn(
        "[Pipeline] ‚ö†Ô∏è Falha ao buscar quest√£o sugerida pelo router:",
        err,
      );
    }
  }

  const finalMessage = message + additionalContextMessage;

  // Notifica sobre mudan√ßa de modo (se foi roteado)
  if (wasRouted && context.onModeDecided) {
    const modeData = {
      mode: finalMode,
      reason: routerResult?.reason,
      confidence: routerResult?.confidence,
      routerResult: routerResult, // Envia tudo explicito
    };

    context.onModeDecided(modeData);

    // [PERSISTENCE] Salvar evento de decis√£o de modo
    if (chatId) {
      ChatStorageService.addMessage(chatId, "system", {
        type: "mode_selected",
        ...modeData,
      }).catch((err) => console.warn("[Pipeline] Erro ao salvar modo:", err));
    }
  }

  // Executa pipeline espec√≠fico
  let systemPrompt;
  let configMode;

  if (finalMode === "scaffolding") {
    systemPrompt = getSystemPromptScaffolding();
    configMode = "scaffolding";
  } else if (finalMode === "raciocinio") {
    systemPrompt = getSystemPromptRaciocinio();
    configMode = "raciocinio";
  } else {
    systemPrompt = getSystemPromptRapido();
    configMode = "rapido";
  }

  console.log(`[Pipeline] üöÄ Executando modo ${finalMode.toUpperCase()}`);

  if (context.onStart) {
    context.onStart({ mode: finalMode });
  }

  try {
    // Acumulador de pensamentos para persist√™ncia
    let accumulatedThoughts = [];

    const fullResponse = await generateChatStreamed({
      model: getModeConfig(configMode).model,
      generationConfig: getGenerationParams(configMode),
      systemPrompt,

      userMessage: finalMessage, // Usa a mensagem com contexto injetado
      attachments,
      onStream: context.onStream, // Callback recebe o objeto estruturado em progresso
      onThought: (thought) => {
        accumulatedThoughts.push(thought);
        if (context.onThought) context.onThought(thought);
      },
      apiKey: context.apiKey,
      chatMode: context.chatMode,
      history: context.history,
      signal: context.signal,
    });

    // A resposta final agora √© o objeto estruturado completo ({ layout, conteudo })
    const finalContent = fullResponse || {};

    // [PERSISTENCE] Anexar pensamentos acumulados ao conte√∫do final para salvar
    if (accumulatedThoughts.length > 0) {
      finalContent._thoughts = accumulatedThoughts;
    }

    if (context.onComplete) {
      context.onComplete({ mode: finalMode, response: finalContent });
    }

    // === PERSISTENCE: SAVE AI RESPONSE ===
    if (chatId) {
      ChatStorageService.addMessage(chatId, "model", finalContent).catch(
        (err) => console.warn("[Pipeline] Erro ao salvar resposta da IA:", err),
      );

      // === AUTO-TITLE GENERATION (If New Chat) ===
      if (isNewChat) {
        generateChatTitleData(message, finalContent, context.apiKey)
          .then((title) => {
            if (title) {
              console.log("[Pipeline] T√≠tulo gerado:", title);
              ChatStorageService.updateTitle(chatId, title);
              if (context.onTitleUpdated) context.onTitleUpdated(chatId, title);
            }
          })
          .catch((err) =>
            console.warn("[Pipeline] Erro ao gerar t√≠tulo:", err),
          );
      }
    }

    // === MEMORY EXTRACTION (ASYNC) ===
    // N√£o aguardamos para n√£o travar a resposta da UI
    setTimeout(() => {
      MemoryService.extractAndSaveNarrative(
        message,
        fullResponse,
        context.apiKey,
        attachments,
      )
        .then(() => console.log("[Pipeline] üß† Ciclo de mem√≥ria conclu√≠do."))
        .catch((err) =>
          console.error("[Pipeline] ‚ö†Ô∏è Erro no ciclo de mem√≥ria:", err),
        );
    }, 100);

    return { success: true, mode: finalMode, response: finalContent };
  } catch (error) {
    // [FIX] Tratamento espec√≠fico para cancelamento (Stop Generation)
    if (error.name === "AbortError" || error.message?.includes("aborted")) {
      console.log("[Pipeline] üõë Execu√ß√£o interrompida pelo usu√°rio.");
      // Repassa o erro de abort para a UI atualizar o estado (bot√£o voltar ao normal)
      if (context.onError) context.onError(error);
      return { success: false, mode: finalMode, aborted: true };
    }

    console.error("[Pipeline] Erro:", error);
    if (context.onError) context.onError(error);
    return { success: false, mode: finalMode, error: error.message };
  }
}

// Stub para manter compatibilidade se algo importar diretamente (n√£o deveria)
export async function runRapidoPipeline(message, attachments, context) {
  return runChatPipeline("rapido", message, attachments, context);
}

export async function runRaciocinioPipeline(message, attachments, context) {
  return runChatPipeline("raciocinio", message, attachments, context);
}

/**
 * Gera um passo de scaffolding silenciosamente (sem atualizar UI principal).
 * Usado para o fluxo de "Slides" do Scaffolding.
 */
export async function generateSilentScaffoldingStep(
  prompt,
  apiKey,
  attachments = [],
) {
  console.log("[Pipeline] ü§´ Gerando passo de scaffolding silencioso...");

  /*
   Reutilizamos generateChatStreamed mas sem callbacks de UI (onStream)
   e com chatMode=false para garantir gera√ß√£o stateless pura baseada no prompt montado.
  */
  const response = await generateChatStreamed({
    model: "gemini-3-flash-preview", // Modelo r√°pido para scaffolding
    generationConfig: {
      responseMimeType: "application/json",
      responseSchema: SCAFFOLDING_STEP_SCHEMA,
    },
    systemPrompt:
      "You are a helpful assistant. Output ONLY valid JSON matching the schema. Do not output multiple JSON objects.", // Strict instruction
    userMessage: prompt,
    attachments,
    onStream: null, // Silencioso
    onThought: null,
    apiKey,
    chatMode: false,
    history: [], // Sem hist√≥rico do servidor, gerenciado manualmente no prompt
  });

  return response;
}

/**
 * Gera resposta usando /generate com streaming e JSON estruturado
 */
async function generateChatStreamed(params) {
  const {
    model,
    generationConfig,
    systemPrompt,
    userMessage,
    attachments = [],
    onStream,
    onThought,
    apiKey,
    chatMode,
    history,
    signal, // Receive signal
  } = params;

  // Monta o prompt combinando system + user com √™nfase no user
  const currentDateTime = new Date().toLocaleString("pt-BR", {
    dateStyle: "full",
    timeStyle: "short",
  });
  const timeContext = `\n[SISTEMA - DATA/HORA ATUAL: ${currentDateTime}]`;

  const fullPrompt = `${systemPrompt}${timeContext}\n\n---\n\n=== PROMPT DO USU√ÅRIO (PRIORIDADE M√ÅXIMA) ===\nUsu√°rio: ${userMessage}\n=== FIM DO PROMPT ===`;

  // Converte anexos (imagens ou arquivos) para base64
  const arquivosProcessados = [];
  // mimeType padr√£o caso seja s√≥ 1 arquivo de imagem (mantendo l√≥gica antiga se necess√°rio, mas agora passamos explicito no objeto)
  let mimeType = "image/jpeg";

  for (const file of attachments) {
    const base64 = await fileToBase64(file);
    arquivosProcessados.push({
      data: base64,
      mimeType: file.type || "application/octet-stream",
    });
    // Se for o primeiro, define o mimeType "principal" (apenas para compatibilidade de assinatura, embora o worker agora use o array de files)
    if (arquivosProcessados.length === 1) {
      mimeType = file.type;
    }
  }

  // Estado local para controle do streaming JSON
  let jsonBuffer = "";
  let lastParsedJson = null;

  // Handlers para o worker
  const handlers = {
    onStatus: (status) => console.log(`[Worker Status] ${status}`),
    onThought: (thought) => {
      if (onThought) onThought(thought);
    },
    onAnswerDelta: (delta) => {
      jsonBuffer += delta;

      // Tenta parsear o que temos at√© agora (best effort)
      const currentParsedAnswer = parseStreamedJSON(jsonBuffer);

      // Se conseguiu extrair um objeto v√°lido (mesmo que parcial)
      if (currentParsedAnswer) {
        // Envia o objeto COMPLETO para a UI a cada update
        // (UI deve ser reativa e redesenhar baseada no estado atual)
        if (onStream) onStream(currentParsedAnswer);
        lastParsedJson = currentParsedAnswer;
      }
    },
    signal, // Pass signal to worker handlers
  };

  const options = {
    model,
    generationConfig,
    chatMode,
    history,
    systemInstruction: chatMode ? systemPrompt : undefined, // In Chat Mode, separate system instruction
  };

  console.log("[Generate] üöÄ Iniciando gera√ß√£o JSON Estruturado Streamed...");

  // Chama a fun√ß√£o do worker
  // Ela retorna o JSON final parseado quando terminar (ou lan√ßa erro)
  const finalJSON = await gerarConteudoEmJSONComImagemStream(
    fullPrompt,
    generationConfig?.responseSchema || CHAT_RESPONSE_SCHEMA,
    arquivosProcessados, // Agora passamos lista de objetos {data, mimeType}
    mimeType,
    handlers,
    options,
  );

  return finalJSON;
}

// fileToBase64 imported from utils

export async function generateChatTitleData(userMsg, aiContent, apiKey) {
  try {
    const aiText =
      typeof aiContent === "string"
        ? aiContent
        : aiContent.conteudo || JSON.stringify(aiContent);

    // Prompt simples e direto para texto puro
    // IMPORTANTE: Gemma N√ÉO pode usar sections, JSON, markdown ou qualquer formata√ß√£o
    const prompt = `
Voc√™ √© um assistente simples. Sua √öNICA tarefa √© gerar um t√≠tulo curto para esta conversa.

REGRAS CR√çTICAS:
- Retorne APENAS o t√≠tulo, nada mais
- N√ÉO use JSON, sections, markdown, aspas ou qualquer formata√ß√£o
- N√ÉO escreva "T√≠tulo:", "Title:", ou prefixos
- N√ÉO use estruturas como { } ou [ ]
- O t√≠tulo deve ter entre 3 a 6 palavras em portugu√™s

EXEMPLOS DE BONS RESULTADOS:
- "An√°lise de Fun√ß√µes Quadr√°ticas"
- "Estrutura do DNA Celular"
- "Revolu√ß√£o Industrial Brasileira"
- "C√°lculo de Derivadas Parciais"
- "Fotoss√≠ntese e Respira√ß√£o Celular"

DADOS DA CONVERSA:
Usu√°rio disse: "${userMsg.substring(0, 200)}"
IA respondeu sobre: "${aiText.substring(0, 200)}"

RESPOSTA (apenas o t√≠tulo, texto puro):`;

    // Usa gerarConteudoEmJSONComImagemStream com modelo Gemma e sem JSON mode for√ßado
    // Necess√°rio adaptar a chamada pois ela espera schemas/attachments
    // Assinatura: (texto, schema, attachments, mimeType, handlers, options)
    let titleAccumulator = "";

    await gerarConteudoEmJSONComImagemStream(
      prompt,
      null, // Schema null
      [], // Attachments empty
      "image/jpeg", // mimeType placeholder
      {
        onAnswerDelta: (text) => {
          titleAccumulator += text;
        },
      },
      {
        model: "gemma-3-27b-it", // Modelo solicitado pelo usu√°rio (Gemma)
        generationConfig: { responseMimeType: "text/plain" }, // For√ßa texto plano
      },
    );

    let title = titleAccumulator;

    // Limpeza garantida
    if (typeof title !== "string") {
      title = JSON.stringify(title);
    }

    // Remove aspas extras e quebras de linha que o modelo possa ter colocado
    title = title
      .replace(/^["']|["']$/g, "")
      .replace(/\n/g, " ")
      .trim();

    // Remove prefixos comuns se houver
    title = title.replace(/^T√≠tulo:\s*/i, "");
    title = title.replace(/^Title:\s*/i, "");

    // Se o modelo gerou JSON apesar das instru√ß√µes, tenta extrair o t√≠tulo
    if (title.startsWith("{") || title.startsWith("[")) {
      try {
        const parsed = JSON.parse(title);
        // Tenta extrair t√≠tulo de estruturas comuns
        title =
          parsed.title ||
          parsed.titulo ||
          parsed.name ||
          parsed.sections?.[0]?.title ||
          JSON.stringify(parsed).substring(0, 50);
      } catch {
        // Se n√£o for JSON v√°lido, pega apenas os primeiros 50 chars
        title = title.replace(/[{}\[\]]/g, "").substring(0, 50);
      }
    }

    // Remove qualquer markdown restante
    title = title.replace(/[#*_`]/g, "").trim();

    return title || "Nova Conversa";
  } catch (e) {
    console.warn("Erro no gerador de t√≠tulos:", e);
    return null;
  }
}
